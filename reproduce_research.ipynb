{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5779d75-7d72-430b-8543-35bd31a64f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6feebe6-056c-4820-a91e-1508780baacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if GPU is available to notebook \n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251beb70-195c-4658-9a15-0295eaaad7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### Data Collection & Processing ##########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f21e80-8d61-497e-98bb-7496b3cafce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install comet-ml ultralytics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baafde06-4a01-43d0-8ed5-101ac32bf365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Hand Detection\n",
    "from comet_ml import Experiment\n",
    "from comet_ml.integration.pytorch import log_model\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "import os\n",
    "import yaml \n",
    "from pathlib import Path\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "def main():\n",
    "    BASE_DIR = Path.cwd()\n",
    "    \n",
    "    with open(BASE_DIR / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    data = BASE_DIR / config[\"data_path\"]\n",
    "\n",
    "    def train(dataset, epochs):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        print(\"Device used: \", device)\n",
    "\n",
    "        save_dir = BASE_DIR / \"yolo_finetuned_weight_output_path\"\n",
    "\n",
    "        model = YOLO(\"yolov8n.pt\").to(device) \n",
    "        model.train(data=dataset, epochs=epochs, workers=0, save_dir=save_dir)\n",
    "        metrics = model.val()  \n",
    "        print(\"Model is using device:\", model.device)\n",
    "\n",
    "    epochs = 200\n",
    "    train(data, epochs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca31c7-d0b7-44c0-a680-ca53b63c7f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now we have a fine-tuned YOLO model under runs/detect/train{?}. There are training details are stored, but most importantly, there is weight under weights/\n",
    "which is the core brain of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1e80b-d390-484c-9dfd-f05ac6a2a37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Hand Segmentation\n",
    "import os\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "def expand_box(box_shape, expansion_factor, frame_width, frame_height):\n",
    "    \"\"\"\n",
    "    A method to expand input box by multipling by expansion_factor.\n",
    "\n",
    "    args:\n",
    "    box_shape(int,int,int,int): box shape left top (x, y) and right bottom (x, y).\n",
    "    expansion_factor(int): expansion factor to expand segmented box.\n",
    "    frame_width(int): orignal frame widht\n",
    "    frame_height(int): original frame height\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box_shape\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    new_x1 = max(0, int(x1 - expansion_factor * width))\n",
    "    new_y1 = max(0, int(y1 - expansion_factor * height))\n",
    "    new_x2 = min(frame_width, int(x2 + expansion_factor * width))\n",
    "    new_y2 = min(frame_height, int(y2 + expansion_factor * height))\n",
    "    return new_x1, new_y1, new_x2, new_y2\n",
    "    \n",
    "def get_target_detection_result(results, target_class):\n",
    "    \"\"\"\n",
    "    A method to extract target class from detection results.\n",
    "\n",
    "    args:\n",
    "    results\n",
    "    target_class(str): target class for detection.\n",
    "    \"\"\"\n",
    "    target_detections = []\n",
    "    for detection in results:\n",
    "        class_indices = detection.boxes.cls.tolist()\n",
    "        if class_indices:  \n",
    "            class_index = int(class_indices[0])  \n",
    "            class_name = detection.names[class_index]  # Get class name\n",
    "    \n",
    "            if class_name == target_class:\n",
    "                target_detections.append(detection)\n",
    "    return target_detections\n",
    "\n",
    "    \n",
    "def segment_hand(model, input_folder, output_folder, expansion_factor = 0.1, target_class = \"left\"):\n",
    "    \"\"\"\n",
    "    A method to segment an object from background by going through each video frame. \n",
    "    To minimize the effect of hand detection failure, segmentation box expansion is implemented,\n",
    "    which keeps the detected hand box even the detection model missed and \"grow\" the box accumulatively \n",
    "    when the object moved out of the previous box area. \n",
    "\n",
    "    args: \n",
    "    target_class(str): a target class to detect and segment.\n",
    "    model(str): hand detection model weight path.\n",
    "    input_folder(str): input video folder path\n",
    "    output_folder(str):  output video folder path\n",
    "    expansion_factor(float): a float number for box expansion factor.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".mp4\"):\n",
    "            input_video_path = os.path.join(input_folder, filename)\n",
    "            output_video_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            cumulative_mask = None\n",
    "\n",
    "            cap = cv.VideoCapture(input_video_path)\n",
    "\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: Could not open input video file {input_video_path}.\")\n",
    "\n",
    "            frame_width = int(cap.get(cv.CAP_PROP_FRAME_WIDTH))\n",
    "            frame_height = int(cap.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
    "            frame_rate = cap.get(cv.CAP_PROP_FPS)\n",
    "\n",
    "            fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv.VideoWriter(output_video_path, fourcc, frame_rate, (frame_width, frame_height))\n",
    "\n",
    "            while True:\n",
    "\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                if not ret:\n",
    "                    break\n",
    "\n",
    "                results = model.predict(frame)\n",
    "                \n",
    "                #frame.shape: [width, height, channel]\n",
    "                mask = np.zeros(frame.shape[:2], dtype=np.uint8)\n",
    "\n",
    "                for result in results:\n",
    "                    target_detections = get_target_detection_result(result, target_class)\n",
    "                    for contour in target_detections:\n",
    "                        box = contour.boxes.xyxy[0] # bounding box coordinates\n",
    "                        expanded_box = expand_box(box, expansion_factor, frame_width, frame_height)\n",
    "                        cv.rectangle(mask, (int(expanded_box[0]),int(expanded_box[1])),(int(expanded_box[2]),int(expanded_box[3])), (255), cv.FILLED)\n",
    "\n",
    "                if cumulative_mask is None:\n",
    "                   cumulative_mask = mask\n",
    "                else:\n",
    "                    CUMULATIVE_MASK_HEIGHT = cumulative_mask.shape[0]\n",
    "                    CUMULATIVE_MASK_WIDTH = cumulative_mask.shape[1]\n",
    "                    resized_mask = cv.resize(mask, (CUMULATIVE_MASK_WIDTH, CUMULATIVE_MASK_HEIGHT))\n",
    "                    cumulative_mask = cv.bitwise_or(cumulative_mask, resized_mask)\n",
    "\n",
    "                masked_frame = cv.bitwise_and(frame, frame, mask = cumulative_mask)\n",
    "                out.write(masked_frame)\n",
    "\n",
    "            cap.release()\n",
    "            out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a3a7a-5cf0-47d1-bf0f-61eb79df3f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import yaml \n",
    "\n",
    "def main():\n",
    "    BASE_DIR = Path.cwd()\n",
    "    \n",
    "    with open(BASE_DIR / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    weights_path = BASE_DIR / config[\"detection_weight_path\"]\n",
    "    input_folder = BASE_DIR / config[\"action_input_data_path\"]\n",
    "    output_folder= BASE_DIR / config[\"segmentation_output_data_path\"]\n",
    "\n",
    "    model = YOLO(weights_path)\n",
    "\n",
    "    for folder in os.listdir(input_folder):\n",
    "        input_file_path = os.path.join(input_folder, folder)\n",
    "        output_file_path = os.path.join(output_folder, folder)\n",
    "        segment_hand(model, input_file_path, output_file_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b8c25a-3521-41d1-9f2e-f97d0dc03863",
   "metadata": {},
   "outputs": [],
   "source": [
    "After this, we have videos that contains segmented hand out of background under output_folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86ee7b-dddc-4aad-bd7a-97b4c1225c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Optical Flow Extraction\n",
    "import cv2 as cv\n",
    "from pathlib import Path\n",
    "import yaml \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "with open(BASE_DIR / \"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "histogram_output_folder_path = BASE_DIR / config[\"histogram_of_motion_output_data_path\"]\n",
    "\n",
    "MAX_FRET_MOVEMENT_THRESHOLD = 18 \n",
    "MIN_MOVEMENT_DISTANCE = 0.01\n",
    "\n",
    "def check_fret_movement(down_direction, up_direction):\n",
    "    \"\"\"\n",
    "    A method to detect the stored movement is due to fret movement, not finger movement.\n",
    "\n",
    "    args:\n",
    "    down_direction(int): \n",
    "    up_direction(int): \n",
    "    return:\n",
    "    boolean: True if fretmovement is detected, otherwise False. \n",
    "    \"\"\"\n",
    "    if down_direction < MAX_FRET_MOVEMENT_THRESHOLD or up_direction < MAX_FRET_MOVEMENT_THRESHOLD:\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "def eliminate_displacement(displacement, threshold):\n",
    "    \"\"\"\n",
    "    A method to eliminate a small displacement by selected threshold.\n",
    "\n",
    "    args: \n",
    "    displacement(int): a size of displacement vector\n",
    "    threshold(int): a int value for thresholding\n",
    "\n",
    "    return:\n",
    "    displacement(int): processed displacement\n",
    "    \"\"\"\n",
    "    if displacement < threshold:\n",
    "        return 0\n",
    "    return displacement\n",
    "    \n",
    "def calculate_direction(old_points, new_points):\n",
    "    \"\"\"\n",
    "    Calculate the direction from an old point to a new point.\n",
    "\n",
    "    param\n",
    "    old_point: Coordinates of the old point as a tuple (x, y).\n",
    "    new_point: Coordinates of the new point as a tuple (x, y).\n",
    "    \n",
    "    return: Angle (in radians) representing the direction from the old point to the new point.\n",
    "    \"\"\"\n",
    "    displacement = np.array(new_points) - np.array(old_points)\n",
    "    distance = np.linalg.norm(displacement)\n",
    "\n",
    "    if distance < MIN_MOVEMENT_DISTANCE:\n",
    "        return distance, 8\n",
    "\n",
    "    direction_rad = np.arctan2(displacement[1], displacement[0])\n",
    "    direction_deg = np.degrees(direction_rad)\n",
    "\n",
    "    if direction_deg < 0:\n",
    "        direction_deg += 360\n",
    "\n",
    "    direction_section = int(direction_deg/45)\n",
    "\n",
    "    return distance, direction_section\n",
    "\n",
    "def create_histgram_of_motion(dictionary, class_name, case_num = None, frame_count = None):\n",
    "    \"\"\"\n",
    "    Create histogram for each direction. x axis is magnitude and y axis is frequencies.\n",
    "    0 : Right\n",
    "    1 : Up Right\n",
    "    2 : Up\n",
    "    3 : Up Left\n",
    "    4 : Left\n",
    "    5 : Down Left\n",
    "    6 : Down\n",
    "    7 : Down Right\n",
    "    8 : No Direction\n",
    "\n",
    "    args:\n",
    "    dictionary(python dict): a python dictionary containing distances for each direction.\n",
    "    class_name(str): a class of the action type\n",
    "    case_num(int): a case number of the video  \n",
    "    frame_count(int): a total number of frames within a video\n",
    "    \"\"\"\n",
    "    for key, distances in dictionary.items():\n",
    "\n",
    "        if len(distances) > 0:\n",
    "            max_distance = max(distances)\n",
    "        else:\n",
    "            max_distance = 0\n",
    "\n",
    "        total_size = frame_count\n",
    "        hist, _ = np.histogram(distances, bins=20, range=(0, max_distance))\n",
    "        relative_frequencies = hist / total_size\n",
    "\n",
    "        plt.bar(np.arange(20), relative_frequencies, color='blue', edgecolor='black', width=0.8)\n",
    "        plt.ylim(0, 1)\n",
    "\n",
    "        os.makedirs(f'{histogram_output_folder_path}/{class_name}/{case_num-1}/', exist_ok=True)\n",
    "        plt.savefig(f'{histogram_output_folder_path}/{class_name}/{case_num-1}/{key}.png')\n",
    "        plt.clf()\n",
    "        \n",
    "    \n",
    "def segmentation_coordinations(frame):\n",
    "    \"\"\"\n",
    "    A method for finding a bounding box (segmented area) information (x, y, w, h) out of black background.\n",
    "\n",
    "    args:\n",
    "    frame(image): an image frame with segmented area and black background. \n",
    "    \n",
    "    return:\n",
    "    a first frame image.\n",
    "    a list of coordinates of the segmentation area.\n",
    "    \"\"\"\n",
    "    frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY) #make the frame greyscale for easier binary threshold\n",
    "    _, binary_image = cv.threshold(frame, 1, 255, cv.THRESH_BINARY)\n",
    "    contours, _  = cv.findContours(binary_image, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours:\n",
    "        segmented_box = max(contours, key=cv.contourArea)\n",
    "        x,y,w,h = cv.boundingRect(segmented_box)\n",
    "        return frame, [x, y, w, h]\n",
    "    \n",
    "def block_features(frame, coordinates, n_blocks):\n",
    "    \"\"\"\n",
    "    A method for generating a block-based feature points to track during optical flow extraction.\n",
    "\n",
    "    Args:\n",
    "        frame (image): An image frame with segmentation.\n",
    "        coordinates ([int]): A list of coordinates of the bounding box (segmentation area) in black background.\n",
    "        n_blocks (int): Number of blocks to set tracking points.\n",
    "\n",
    "    Returns:\n",
    "        bool: Boolean value to check if the process was successful.\n",
    "        np.ndarray: Numpy array containing feature point coordinates.\n",
    "    \"\"\"\n",
    "    boo = False\n",
    "    result = None  \n",
    "    try:\n",
    "        start_x, start_y, width, height = coordinates\n",
    "\n",
    "        x = int(width / (n_blocks + 1))\n",
    "        y = int(height / (n_blocks + 1))\n",
    "\n",
    "        result = []\n",
    "        for row in range(n_blocks):\n",
    "            for col in range(n_blocks):\n",
    "                box_point_x = int(start_x + (col + 1) * x)\n",
    "                box_point_y = int(start_y + (row + 1) * y)\n",
    "                result.append([np.float32(box_point_x), np.float32(box_point_y)])\n",
    "\n",
    "        result = np.array(result)\n",
    "\n",
    "        if result.shape[0] == n_blocks * n_blocks:\n",
    "            result = result.reshape(n_blocks * n_blocks, 1, 2)\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected number of feature points in result.\")\n",
    "\n",
    "        boo = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error at Block Feature Generation function:\", e)\n",
    "\n",
    "        debug_mode = True\n",
    "        if debug_mode:\n",
    "            cv.imshow(\"error\", frame)\n",
    "            cv.waitKey(0)\n",
    "            cv.destroyAllWindows()\n",
    "\n",
    "    finally:\n",
    "        return boo, result\n",
    "                    \n",
    "def extract_optical_flow(input_folder, output_folder, class_name, size=None, threshold=2):\n",
    "    \"\"\"\n",
    "    A method for processing a video to draw/track optical flow. \n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    if(size is None):\n",
    "        size = len(os.listdir(input_folder))\n",
    "\n",
    "    #Lukas-kanade optical flow parameters\n",
    "    lk_params = dict(winSize=(50,50), maxLevel=2, criteria=(cv.TERM_CRITERIA_EPS | cv.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "    color = (0, 255, 0)\n",
    "    dictionary = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]} #directions 0 - 8 which is top, right top, ... etc.\n",
    "    frame_count = 0\n",
    "    num_videos_processed = 0\n",
    "    case_num = 0\n",
    "\n",
    "    while num_videos_processed < size:\n",
    "        video_list = os.listdir(input_folder)\n",
    "        for video_file_name in video_list:\n",
    "            video_file_path = os.path.join(input_folder, video_file_name)\n",
    "            cap = cv.VideoCapture(video_file_path)\n",
    "\n",
    "            if not cap.isOpened():\n",
    "                print(f\"Error: Could not open {video_file_path}\")\n",
    "                case_num = case_num - 1\n",
    "                continue\n",
    "\n",
    "            fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
    "            output_video_path = os.path.join(output_folder, f\"optical_flow_{video_file_name}\")\n",
    "            output_writer = cv.VideoWriter(output_video_path, fourcc, 20.0, (640, 480))\n",
    "\n",
    "            ok, frame = cap.read()\n",
    "\n",
    "            if np.all(frame==0):\n",
    "                print(\"frame skipped due to mal-detection. No hand segmentation was happened on this frame.\")\n",
    "                continue\n",
    "\n",
    "            # prev_gray = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "            \n",
    "            first_grayscale_frame, coordinates = segmentation_coordinations(frame)\n",
    "\n",
    "            boo, first_feature_points = block_features(frame, coordinates, 6)\n",
    "\n",
    "            mask = np.zeros_like(frame)\n",
    "\n",
    "            while cap.isOpened():\n",
    "                ok, frame = cap.read()\n",
    "                \n",
    "                if not ok:\n",
    "                    break\n",
    "                    \n",
    "                frame_count += 1\n",
    "                \n",
    "                if np.all(frame==0):\n",
    "                    prinit(\"Blank frame is detected\")\n",
    "                    continue\n",
    "                    \n",
    "                cap.set(cv.CAP_PROP_FRAME_WIDTH, 640)\n",
    "                cap.set(cv.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "                next_grayscale_frame = cv.cvtColor(frame, cv.COLOR_BGR2GRAY)\n",
    "\n",
    "                # calculates sparse optical flow by Lucas-Kanade method\n",
    "                next_feature_points, status, error = cv.calcOpticalFlowPyrLK(first_grayscale_frame, next_grayscale_frame, first_feature_points, None, **lk_params)\n",
    "                good_old = first_feature_points[status==1]\n",
    "                good_new = next_feature_points[status==1]\n",
    "\n",
    "                direction_detector = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0} #used for fret movement elimination (hopefully removed later improvement)\n",
    "                sub_dictionary = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]}\n",
    "\n",
    "                for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                    new_x, new_y = new.ravel()\n",
    "                    old_x, old_y = old.ravel()\n",
    "\n",
    "                    displacement, direction = calculate_direction((old_x, old_y), (new_x, new_y))\n",
    "\n",
    "                    displacement = eliminate_displacement(displacement, threshold)\n",
    "\n",
    "                    if displacement != 0: \n",
    "                        direction_detector[direction] = direction_detector[direction]+1\n",
    "                        sub_dictionary[direction].append(displacement)\n",
    "\n",
    "                    mask_with_flow = cv.line(mask, (int(new_x), int(new_y)), (int(old_x),int(old_y)), color, 1)\n",
    "                    frame = cv.circle(frame, (int(new_x), int(new_y)), 1, color, -1)\n",
    "\n",
    "                if not check_fret_movement(direction_detector[2], direction_detector[6]):\n",
    "                    for key, values in sub_dictionary.items():\n",
    "                        for element in values:\n",
    "                            dictionary[key].append(element)\n",
    "\n",
    "                #update frame and feature points to next frame info\n",
    "                first_grayscale_frame = next_grayscale_frame.copy()\n",
    "                first_feature_points = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "                #write processed frame \n",
    "                output_frame = cv.add(frame, mask)\n",
    "                output_writer.write(output_frame)\n",
    "\n",
    "            # Release the resources\n",
    "            cap.release()\n",
    "            output_writer.release()\n",
    "            cv.destroyAllWindows()\n",
    "\n",
    "            print(f\"Sparse optical flow video saved: {output_video_path}\")\n",
    "                    \n",
    "            num_videos_processed += 1\n",
    "            case_num += 1\n",
    "\n",
    "            # create histogram for each direction\n",
    "            print(\"Iteration at: \", class_name)\n",
    "            print(\"Current Number: \",num_videos_processed)\n",
    "            print(\"case number: \", case_num)\n",
    "            create_histgram_of_motion(dictionary, class_name, case_num, frame_count)\n",
    "\n",
    "            # Initialize dictionary and frame count for next video\n",
    "            dictionary = {0:[],1:[],2:[],3:[],4:[],5:[],6:[],7:[],8:[]}\n",
    "            frame_count = 0  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4575629f-d413-4288-a251-00358a10c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml \n",
    "import os \n",
    "\n",
    "def main():\n",
    "    BASE_DIR = Path.cwd()\n",
    "    \n",
    "    with open(BASE_DIR / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    input_folder_path = BASE_DIR / config[\"segmentation_output_data_path\"]\n",
    "    output_folder_path = BASE_DIR / config[\"optical_flow_output_data_path\"]\n",
    "    \n",
    "    for folder in os.listdir(input_folder_path):\n",
    "        if(folder != \"norm\"):\n",
    "            input_folder = os.path.join(input_folder_path, folder)\n",
    "            output_folder = os.path.join(output_folder_path, folder)\n",
    "            extract_optical_flow(input_folder, output_folder, folder)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b212ea9-84a5-4884-b28a-4b1938cfbb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, all preprocessing process is done. We have acess to optical flow data under datasets/actions_dataset/data/optical_flow,\n",
    "and histogram of motion for each action types under datasets/histgram_of_motion/. Each action contains multiple examples, \n",
    "and each example contains 9 histograms of motion in total for corresponding direction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c73cc-c2bc-4f13-affa-e39cfa3332eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### Classification Model ###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71c8dd3-4348-4331-b99d-ad8ef2a85cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "We have two choices to make classification models: 1. Use histogram images for trainnig and 2. Use histogram tensors.\n",
    "I actually used histogram images due to my technical drawbacks at that time I was buildinig for my research, \n",
    "but now I realized \"why not use the histogram tensor directly?\". It is more natural to use it after learning \n",
    "more about these stuff... (I hope I get more time for techncial discussions with peers next time I can collaborate with someone.)\n",
    "I firstly present the revised original version, which is image-based classifier. \n",
    "(Btw, I even needed to revise this implementation from the original one to make sure my implementation is correct.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e490f-320f-4564-a68c-c30bd8ddc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Build a multi-stream LeNet model (Using Histogram Images)\n",
    "#Revised\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module, Conv2d, Linear, MaxPool2d, ReLU, ModuleList, LogSoftmax\n",
    "from torch import flatten\n",
    "\n",
    "class LeNet(Module):\n",
    "    def __init__(self, numChannels, feature_size=500):\n",
    "        super(LeNet, self).__init__()\n",
    "\n",
    "        self.conv1 = Conv2d(in_channels=numChannels, out_channels=20, kernel_size=(5,5))\n",
    "        self.relu1 = ReLU()\n",
    "        self.maxpool1 = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "        self.conv2 = Conv2d(in_channels=20, out_channels=50, kernel_size=(5,5))\n",
    "        self.relu2 = ReLU()\n",
    "        self.maxpool2 = MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "\n",
    "        self.fc1 = Linear(in_features=1250, out_features=feature_size)\n",
    "        self.relu3 = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        \n",
    "        x = flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiStreamCNN(Module):\n",
    "    def __init__(self, numChannels, num_classes, feature_size=500):\n",
    "        super(MultiStreamCNN, self).__init__()\n",
    "        \n",
    "        self.streams = ModuleList([\n",
    "            LeNet(numChannels, feature_size) for _ in range(9)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = Linear(feature_size * 9, num_classes)\n",
    "        \n",
    "    def forward(self, histograms):\n",
    "        \n",
    "        features = []\n",
    "        for i, histogram in enumerate(histograms):\n",
    "            feature = self.streams[i](histogram)\n",
    "            features.append(feature)\n",
    "        \n",
    "        combined_features = torch.cat(features, dim=1)\n",
    "        \n",
    "        output = self.classifier(combined_features)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "model = MultiStreamCNN(numChannels=1, num_classes=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8000fd58-80c5-44a3-8037-45d79b38ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### Training & Testing & Inference #####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c038dd-d0ab-4cbb-9d46-cd6f43133e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998cdbf1-520e-431c-ac57-c2aa2547ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Training & Testing\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, Grayscale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = self._load_samples()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for cls in self.classes:\n",
    "            class_dir = os.path.join(self.root_dir, cls)\n",
    "            for case_dir in os.listdir(class_dir):\n",
    "                case_path = os.path.join(class_dir, case_dir)\n",
    "                histograms = self._load_histograms(case_path)\n",
    "                samples.append((histograms, self.class_to_idx[cls]))\n",
    "        return samples\n",
    "\n",
    "    def _load_histograms(self, case_path):\n",
    "        histograms = []\n",
    "        for i in range(9):\n",
    "            hist_path = os.path.join(case_path, f\"{i}.png\")\n",
    "            histogram = Image.open(hist_path)\n",
    "            histograms.append(histogram)\n",
    "        return histograms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        histograms, label = self.samples[idx]\n",
    "        if self.transform:\n",
    "            histograms = [self.transform(hist) for hist in histograms]\n",
    "        return histograms, label\n",
    "\n",
    "def evaluate(model_path, val_loader, num_classes):\n",
    "    \"\"\"\n",
    "    This function evaluates the model performance on the data_loader.\n",
    "\n",
    "    Args:\n",
    "        model: The multistream CNN model.\n",
    "        data_loader: The data loader for the evaluation set.\n",
    "        device: The device (\"cuda\" or \"cpu\") to use for computation.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing accuracy and loss (optional, depending on your needs).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "    state_dict = torch.load(model_path)\n",
    "\n",
    "    model = MultiStreamCNN(numChannels=1, num_classes=num_classes)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    data_loader = val_loader\n",
    "    model.eval()  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    loss_val = 0.0\n",
    "    all_predictions = []  \n",
    "    all_ground_truths = []  \n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for i, (hist_streams, labels) in enumerate(data_loader):\n",
    "            hist_streams = [stream.to(device) for stream in hist_streams]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(hist_streams)\n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            num_samples += labels.size(0)\n",
    "            loss_val += criterion(outputs, labels).item()\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())  \n",
    "            all_ground_truths.extend(labels.cpu().numpy())\n",
    "            \n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        all_ground_truths, all_predictions, average=None)  \n",
    "\n",
    "    print(f'Evaluation Accuracy: {accuracy:.4f}')\n",
    "    for i in range(len(precision)):\n",
    "        print(f'Class {i}:')\n",
    "        print(f'  Precision: {precision[i]:.4f}')\n",
    "        print(f'  Recall: {recall[i]:.4f}')\n",
    "        print(f'  F1-Score: {f1_score[i]:.4f}')\n",
    "        print()\n",
    "\n",
    "    print(\"##########################################################################\\n\")\n",
    "    print()\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        all_ground_truths, all_predictions, average=\"weighted\")  \n",
    "\n",
    "    print(f'Evaluation Precision (Weighted Average): {precision:.4f}')\n",
    "    print(f'Evaluation Recall (Weighted Average): {recall:.4f}')\n",
    "    print(f'Evaluation F1-Score (Weighted Average): {f1_score:.4f}')\n",
    "\n",
    "    \n",
    "def train(dataset_root, output_root, classes):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 40\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    transform = Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        Grayscale(num_output_channels=1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    dataset = CustomDataset(root_dir = dataset_root, classes = classes, transform = transform)\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(dataset, dataset.samples, test_size=0.30, random_state=42)\n",
    "    num_train_samples = len(train_labels)\n",
    "    num_val_samples = len(val_labels)\n",
    "\n",
    "    print(f'Number of training samples: {num_train_samples}')\n",
    "    print(f'Number of validation samples: {num_val_samples}')\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)  \n",
    "\n",
    "    model = MultiStreamCNN(numChannels=1, num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss = []  \n",
    "    train_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (hist_streams, labels) in enumerate(train_loader):\n",
    "            \n",
    "            hist_streams = [stream.to(device) for stream in hist_streams]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(hist_streams)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for hist_streams, labels in train_loader:\n",
    "                    hist_streams = [stream.to(device) for stream in hist_streams]\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(hist_streams)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                train_accuracy.append(correct / total)\n",
    "\n",
    "    model_path = os.path.join(output_root, \"multi_stream_cnn.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    plt.subplot(1, 2, 1)  \n",
    "    plt.plot(train_loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "\n",
    "    plt.subplot(1, 2, 2)  \n",
    "    plt.plot(train_accuracy)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Accuracy')\n",
    "    plt.title('Training Accuracy Curve')\n",
    "\n",
    "    plt.tight_layout()  \n",
    "\n",
    "    plt.savefig(f\"{output_root}/train_mult_loss_acc_with_norm.png\")  \n",
    "\n",
    "    evaluate(model_path, val_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c188f1-287b-46ac-87f8-c0f8858cedfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml \n",
    "import os \n",
    "\n",
    "def main():\n",
    "    BASE_DIR = Path.cwd()\n",
    "    \n",
    "    with open(BASE_DIR / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    dataset_root = BASE_DIR / config[\"histogram_of_motion_output_data_path\"]\n",
    "    output_root = BASE_DIR / config[\"model_weight_output_path\"]\n",
    "    classes = [\"ham\", \"pull\", \"slide\"]\n",
    "    train(dataset_root, output_root, classes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd7fbf1-c845-4aa8-ad3b-d9e649b28226",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57682e-de87-4faa-92c4-e2b4154ef3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Summary\n",
    "from torchsummary import summary\n",
    "\n",
    "def custom_summary(model, input_size):\n",
    "  \"\"\"\n",
    "  Prints a summary of the MultiStreamCNN model architecture.\n",
    "\n",
    "  Args:\n",
    "      model: The MultiStreamCNN model instance.\n",
    "      input_size: A tuple representing the input size (e.g., (num_channels, 32, 32)).\n",
    "  \"\"\"\n",
    "\n",
    "  lenet = model.streams[0]  \n",
    "\n",
    "  print(\"---------- LeNet Summary ----------\")\n",
    "  print(f\"Input Size: {input_size}\")\n",
    "\n",
    "  total_params_lenet = 0\n",
    "  for name, param in lenet.named_parameters():\n",
    "    if 'bias' in name:\n",
    "      params = param.numel()\n",
    "    else:\n",
    "      params = param.numel() * param.size(1)  \n",
    "    total_params_lenet += params\n",
    "    print(f\"{name}: {params} parameters\")\n",
    "\n",
    "  print(f\"Total Parameters (LeNet): {total_params_lenet}\")\n",
    "\n",
    "  print(\"\\n---------- MultiStreamCNN Summary ----------\")\n",
    "  total_params_multistream = total_params_lenet * len(model.streams)  \n",
    "  total_params_multistream += model.classifier.in_features * model.classifier.out_features  \n",
    "  print(f\"Total Trainable Parameters: {total_params_multistream}\")\n",
    "\n",
    "  print(\"\\n---------- Model Layers ----------\")\n",
    "  for name, _ in model.named_children():\n",
    "    print(name)\n",
    "\n",
    "  print(\"-\" * 80)  \n",
    "\n",
    "model = MultiStreamCNN(numChannels=1, num_classes=3)\n",
    "custom_summary(model, input_size=(1, 32, 32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1317fc99-bca3-45c1-b171-92122d377c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Inference (Maybe Implement later)\n",
    "class CustomDatasetForPrediction(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.samples = self._load_samples()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        case_path = self.root_dir\n",
    "        histograms = self._load_histograms(case_path)\n",
    "        samples.append(histograms)\n",
    "        return samples\n",
    "\n",
    "    def _load_histograms(self, case_path):\n",
    "        histograms = []\n",
    "        for i in range(9):\n",
    "            hist_path = os.path.join(case_path, f\"{i}.png\")\n",
    "            histogram = Image.open(hist_path)\n",
    "            histograms.append(histogram)\n",
    "        return histograms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        histograms = self.samples[idx]\n",
    "        if self.transform:\n",
    "            histograms = [self.transform(hist) for hist in histograms]\n",
    "        return histograms\n",
    "\n",
    "def predict(folder_path, num_classes):\n",
    "    \"\"\"\n",
    "    This function predicts labels for data in the specified folder using a trained multi-stream CNN model.\n",
    "\n",
    "    Args:\n",
    "        model_path: The path to the saved model checkpoint.\n",
    "        folder_path: The path to the folder containing data for prediction.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing accuracy, precision, recall, and F1-score.\n",
    "    \"\"\"\n",
    "    model_path=\"./model/multi_stream_cnn.pth\"\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    state_dict = torch.load(model_path)\n",
    "\n",
    "    model = MultiStreamCNN(numChannels=1, num_classes=num_classes) \n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    transform = Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        Grayscale(num_output_channels=1),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    dataset = CustomDatasetForPrediction(folder_path, transform=transform)\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "    model.eval()  \n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for i, hist_streams in enumerate(data_loader):\n",
    "            hist_streams = [stream.to(device) for stream in hist_streams]\n",
    "\n",
    "            outputs = model(hist_streams)\n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "    return predicted\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0359eeb2-d5f0-421d-930f-72850fa54ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "We need to modify the optical flow extraction part since we only saved histogram images from \n",
    "the previous implementation. We actually need to save the histogram of motion for each corresponding \n",
    "direction as pytorch tensor and also need to build a different model and data loader \n",
    "for training/testing/evaluation process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4697d-af68-43b8-b685-8cee537605d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revised version of histogram extraction method aka we store histogram tensor at the end.\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml \n",
    "import numpy as np \n",
    "\n",
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "with open(BASE_DIR / \"config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "histogram_output_folder_path = BASE_DIR / config[\"histogram_of_motion_output_tensor_data_path\"]\n",
    "\n",
    "HISTOGRAM_BIN_SIZE=20\n",
    "FIXED_MAX_DISTANCE=10.0\n",
    "def create_histgram_of_motion(dictionary, class_name, case_num = None, frame_count = None):\n",
    "    \"\"\"\n",
    "    Create histogram for each direction. x axis is magnitude and y axis is frequencies.\n",
    "    Stores histogram tensor. \n",
    "    0 : Right\n",
    "    1 : Up Right\n",
    "    2 : Up\n",
    "    3 : Up Left\n",
    "    4 : Left\n",
    "    5 : Down Left\n",
    "    6 : Down\n",
    "    7 : Down Right\n",
    "    8 : No Direction\n",
    "\n",
    "    args:\n",
    "    dictionary(python dict): a python dictionary containing distances for each direction.\n",
    "    class_name(str): a class of the action type\n",
    "    case_num(int): a case number of the video  \n",
    "    frame_count(int): a total number of frames within a video\n",
    "    \"\"\"\n",
    "    for key, distances in dictionary.items():\n",
    "\n",
    "        if len(distances) > 0:\n",
    "            max_distance = max(distances)\n",
    "        else:\n",
    "            max_distance = 1e-6 \n",
    "            \n",
    "        hist, _ = np.histogram(distances, bins=HISTOGRAM_BIN_SIZE, range=(0, max_distance))\n",
    "\n",
    "        total_count = hist.sum()\n",
    "        if total_count > 0:\n",
    "            normalized_hist = hist / total_count\n",
    "        else:\n",
    "            normalized_hist = np.zeros_like(hist, dtype=np.float32)\n",
    "\n",
    "        output_dir = f'{histogram_output_folder_path}/{class_name}/{case_num-1}/'\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        np.save(f'{output_dir}/{key}.npy', normalized_hist.astype(np.float32))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127ec6f5-21fb-4a55-bd66-7e2f82fabf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we gonna run optical flow extraction again...\n",
    "\n",
    "from pathlib import Path\n",
    "import yaml \n",
    "import os \n",
    "\n",
    "def main():\n",
    "    BASE_DIR = Path.cwd()\n",
    "    \n",
    "    with open(BASE_DIR / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "\n",
    "    input_folder_path = BASE_DIR / config[\"segmentation_output_data_path\"]\n",
    "    output_folder_path = BASE_DIR / config[\"optical_flow_output_data_path\"]\n",
    "    \n",
    "    for folder in os.listdir(input_folder_path):\n",
    "        if(folder != \"norm\"):\n",
    "            input_folder = os.path.join(input_folder_path, folder)\n",
    "            output_folder = os.path.join(output_folder_path, folder)\n",
    "            extract_optical_flow(input_folder, output_folder, folder)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51b73d8-a657-482b-973c-f466bb37afed",
   "metadata": {},
   "outputs": [],
   "source": [
    "Then, a new version of model using tensors... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c58456f-8394-4df6-9e58-0011a25b6690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Build a Multi-Stream CNN (Using histgram tensors)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class HistogramProcessor(nn.Module):\n",
    "    def __init__(self, hist_size, feature_size=128):\n",
    "        \"\"\"\n",
    "        Process a 1D histogram directly\n",
    "        \n",
    "        Args:\n",
    "            hist_size: Size of the input histogram (number of bins)\n",
    "            feature_size: Size of the output feature vector\n",
    "        \"\"\"\n",
    "        super(HistogramProcessor, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=16, kernel_size=5, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=16, out_channels=32, kernel_size=5, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        conv_output_size = hist_size // 4 * 32  \n",
    "        \n",
    "        self.fc1 = nn.Linear(conv_output_size, feature_size)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, hist_size]\n",
    "        # Add channel dimension\n",
    "        x = x.unsqueeze(1)  # [batch_size, 1, hist_size]\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MultiStreamHistogramCNN(nn.Module):\n",
    "    def __init__(self, hist_size, num_classes, feature_size=128):\n",
    "        \"\"\"\n",
    "        Multi-stream CNN for processing 9 directional histograms\n",
    "        \n",
    "        Args:\n",
    "            hist_size: Size of each histogram (number of bins)\n",
    "            num_classes: Number of output classes\n",
    "            feature_size: Size of feature vectors from each stream\n",
    "        \"\"\"\n",
    "        super(MultiStreamHistogramCNN, self).__init__()\n",
    "        \n",
    "        self.streams = nn.ModuleList([\n",
    "            HistogramProcessor(hist_size, feature_size) for _ in range(9)\n",
    "        ])\n",
    "        \n",
    "        self.classifier = nn.Linear(feature_size * 9, num_classes)\n",
    "        \n",
    "    def forward(self, histograms):\n",
    "        \"\"\"\n",
    "        Process 9 directional histograms\n",
    "        \n",
    "        Args:\n",
    "            histograms: List of 9 tensors, each with shape [batch_size, hist_size]\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        for i, histogram in enumerate(histograms):\n",
    "            feature = self.streams[i](histogram)\n",
    "            features.append(feature)\n",
    "        \n",
    "        combined = torch.cat(features, dim=1)\n",
    "        \n",
    "        output = self.classifier(combined)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "hist_size = HISTOGRAM_BIN_SIZE  \n",
    "model = MultiStreamHistogramCNN(hist_size=hist_size, num_classes=3)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ca4e79-1b36-419e-a4e7-94204aa499b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################### Training & Testing #####################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae81144-bd9f-4689-9497-f1e42ee88c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Training & Testing\n",
    "import torch \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch import nn, optim\n",
    "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, Grayscale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np \n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, classes, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = classes\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.samples = self._load_samples()\n",
    "        self.transform = transform\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for cls in self.classes:\n",
    "            class_dir = os.path.join(self.root_dir, cls)\n",
    "            for case_dir in os.listdir(class_dir):\n",
    "                case_path = os.path.join(class_dir, case_dir)\n",
    "                histograms = self._load_histograms(case_path)\n",
    "                samples.append((histograms, self.class_to_idx[cls]))\n",
    "        return samples\n",
    "\n",
    "    def _load_histograms(self, case_path):\n",
    "        histograms = []\n",
    "        for i in range(9):\n",
    "            hist_path = os.path.join(case_path, f\"{i}.npy\")\n",
    "            histogram = np.load(hist_path)\n",
    "            histograms.append(histogram)\n",
    "        return histograms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        histograms, label = self.samples[idx]\n",
    "        if self.transform:\n",
    "            histograms = [self.transform(hist) for hist in histograms]\n",
    "        return histograms, label\n",
    "\n",
    "def evaluate(model_path, val_loader, num_classes):\n",
    "    \"\"\"\n",
    "    This function evaluates the model performance on the data_loader.\n",
    "\n",
    "    Args:\n",
    "        model: The multistream CNN model.\n",
    "        data_loader: The data loader for the evaluation set.\n",
    "        device: The device (\"cuda\" or \"cpu\") to use for computation.\n",
    "\n",
    "    Returns:\n",
    "        A tuple containing accuracy and loss (optional, depending on your needs).\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "    state_dict = torch.load(model_path)\n",
    "\n",
    "    model = MultiStreamHistogramCNN(hist_size=HISTOGRAM_BIN_SIZE, num_classes=num_classes)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    data_loader = val_loader\n",
    "    model.eval()  \n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    loss_val = 0.0\n",
    "    all_predictions = []  \n",
    "    all_ground_truths = []  \n",
    "        \n",
    "    with torch.no_grad():  \n",
    "        for i, (hist_streams, labels) in enumerate(data_loader):\n",
    "            hist_streams = [stream.to(device) for stream in hist_streams]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(hist_streams)\n",
    "            _, predicted = torch.max(outputs.data, 1)  \n",
    "            num_correct += (predicted == labels).sum().item()\n",
    "            num_samples += labels.size(0)\n",
    "            loss_val += criterion(outputs, labels).item()\n",
    "\n",
    "            all_predictions.extend(predicted.cpu().numpy())  \n",
    "            all_ground_truths.extend(labels.cpu().numpy())\n",
    "            \n",
    "    accuracy = num_correct / num_samples\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        all_ground_truths, all_predictions, average=None)  \n",
    "\n",
    "    print(f'Evaluation Accuracy: {accuracy:.4f}')\n",
    "    for i in range(len(precision)):\n",
    "        print(f'Class {i}:')\n",
    "        print(f'  Precision: {precision[i]:.4f}')\n",
    "        print(f'  Recall: {recall[i]:.4f}')\n",
    "        print(f'  F1-Score: {f1_score[i]:.4f}')\n",
    "        print()\n",
    "\n",
    "    print(\"##########################################################################\\n\")\n",
    "    print()\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(\n",
    "        all_ground_truths, all_predictions, average=\"weighted\")  \n",
    "\n",
    "    print(f'Evaluation Precision (Weighted Average): {precision:.4f}')\n",
    "    print(f'Evaluation Recall (Weighted Average): {recall:.4f}')\n",
    "    print(f'Evaluation F1-Score (Weighted Average): {f1_score:.4f}')\n",
    "\n",
    "def train(dataset_root, output_root, classes):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    batch_size = 64\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 50\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    dataset = CustomDataset(root_dir = dataset_root, classes = classes)\n",
    "    train_data, val_data, train_labels, val_labels = train_test_split(dataset, dataset.samples, test_size=0.30, random_state=42)\n",
    "    num_train_samples = len(train_labels)\n",
    "    num_val_samples = len(val_labels)\n",
    "\n",
    "    print(f'Number of training samples: {num_train_samples}')\n",
    "    print(f'Number of validation samples: {num_val_samples}')\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)  \n",
    "\n",
    "    model = MultiStreamHistogramCNN(hist_size=HISTOGRAM_BIN_SIZE, num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_loss = []  \n",
    "    train_accuracy = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (hist_streams, labels) in enumerate(train_loader):\n",
    "            \n",
    "            hist_streams = [stream.to(device) for stream in hist_streams]\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(hist_streams)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            if (i+1) % 100 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                correct = 0\n",
    "                total = 0\n",
    "                for hist_streams, labels in train_loader:\n",
    "                    hist_streams = [stream.to(device) for stream in hist_streams]\n",
    "                    labels = labels.to(device)\n",
    "                    outputs = model(hist_streams)\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "                train_accuracy.append(correct / total)\n",
    "\n",
    "    model_path = os.path.join(output_root, \"multi_stream_histogram_cnn.pth\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    plt.subplot(1, 2, 1)  \n",
    "    plt.plot(train_loss)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Loss')\n",
    "    plt.title('Training Loss Curve')\n",
    "\n",
    "    plt.subplot(1, 2, 2)  \n",
    "    plt.plot(train_accuracy)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Training Accuracy')\n",
    "    plt.title('Training Accuracy Curve')\n",
    "    plt.tight_layout()  \n",
    "\n",
    "    plt.savefig(f\"{output_root}/train_mult_hist_loss_acc_with_norm.png\")  \n",
    "\n",
    "    evaluate(model_path, val_loader, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c466c1b-be4c-4e17-8f00-dcdff950ff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import yaml \n",
    "import os \n",
    "\n",
    "def main():\n",
    "    BASE_DIR = Path.cwd()\n",
    "    \n",
    "    with open(BASE_DIR / \"config.yaml\", \"r\") as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    dataset_root = BASE_DIR / config[\"histogram_of_motion_output_tensor_data_path\"]\n",
    "    output_root = BASE_DIR / config[\"model_weight_output_path\"]\n",
    "    classes = [\"ham\", \"pull\", \"slide\"]\n",
    "    train(dataset_root, output_root, classes)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d2716b-16a7-457a-a785-cbc69e0f8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Summary\n",
    "from torchsummary import summary\n",
    "\n",
    "def custom_summary(model, input_size):\n",
    "  \"\"\"\n",
    "  Prints a summary of the MultiStreamCNN model architecture.\n",
    "\n",
    "  Args:\n",
    "      model: The MultiStreamCNN model instance.\n",
    "      input_size: A tuple representing the input size (e.g., (num_channels, 32, 32)).\n",
    "  \"\"\"\n",
    "\n",
    "  lenet = model.streams[0]  \n",
    "\n",
    "  print(\"---------- LeNet Summary ----------\")\n",
    "  print(f\"Input Size: {input_size}\")\n",
    "\n",
    "  total_params_lenet = 0\n",
    "  for name, param in lenet.named_parameters():\n",
    "    if 'bias' in name:\n",
    "      params = param.numel()\n",
    "    else:\n",
    "      params = param.numel() * param.size(1)  \n",
    "    total_params_lenet += params\n",
    "    print(f\"{name}: {params} parameters\")\n",
    "\n",
    "  print(f\"Total Parameters (LeNet): {total_params_lenet}\")\n",
    "\n",
    "  print(\"\\n---------- MultiStreamCNN Summary ----------\")\n",
    "  total_params_multistream = total_params_lenet * len(model.streams)  \n",
    "  total_params_multistream += model.classifier.in_features * model.classifier.out_features  \n",
    "  print(f\"Total Trainable Parameters: {total_params_multistream}\")\n",
    "\n",
    "  print(\"\\n---------- Model Layers ----------\")\n",
    "  for name, _ in model.named_children():\n",
    "    print(name)\n",
    "\n",
    "  print(\"-\" * 80)  \n",
    "\n",
    "\n",
    "model = MultiStreamHistogramCNN(hist_size=HISTOGRAM_BIN_SIZE, num_classes=3)\n",
    "custom_summary(model, input_size=(1, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a6bf4f-de9a-41ec-a502-29d9cb8cb830",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################ Furthermore ##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1401d-ef9b-4fb7-a7cb-68267a473f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Note myself:\n",
    "- try mediapipe for optical extraction feature for better action data retrieval\n",
    "- try more data\n",
    "- try vision transformer for better recogniton\n",
    "- try applying multi-head attention to finger movement\n",
    "- try "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CV (GPU)",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
